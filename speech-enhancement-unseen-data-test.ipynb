{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This Notebook is the second part of Speech Enhancement project with Tensorflow, Please see https://www.kaggle.com/tariqblecher/speech-enhancement-tensorflow-unet-softmask for more information.\n\nContact : tariq.blecher@gmail.com","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"pip install pypesq","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:37:12.116170Z","iopub.execute_input":"2022-07-17T07:37:12.116681Z","iopub.status.idle":"2022-07-17T07:37:24.637140Z","shell.execute_reply.started":"2022-07-17T07:37:12.116632Z","shell.execute_reply":"2022-07-17T07:37:24.636149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport sys\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio\nimport tensorflow as tf\nimport tensorflow_io as tfio\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nimport keras\nfrom keras.models import Sequential\nimport tensorflow_io as tfio\nimport warnings\nimport glob\nimport warnings\nimport os\nimport time\nimport datetime\nfrom pypesq import pesq\nimport soundfile as sf\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-17T07:37:24.639422Z","iopub.execute_input":"2022-07-17T07:37:24.639846Z","iopub.status.idle":"2022-07-17T07:37:24.650210Z","shell.execute_reply.started":"2022-07-17T07:37:24.639809Z","shell.execute_reply":"2022-07-17T07:37:24.649170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"sr=8000\nspeech_length_pix_sec=27e-3\ntotal_length = 3.6\ntrim_length = 28305\nn_fft=255\nframe_length=255\nframe_step = 110\n\nnoisefiles = glob.glob('/kaggle/input/hospital-ambient-noise/Hospita*original/**/*.wav',recursive=True)\nfiles= glob.glob('/kaggle/input/speech-accent-archive/**/*.mp3',recursive=True)\n\nprint(len(files),'clean data files')\nprint(len(noisefiles),'noise data files')\n\n\nfrom numpy.random import MT19937\nfrom numpy.random import RandomState, SeedSequence\nrs = RandomState(MT19937(SeedSequence(123456789)))\n\ndef white_noise(data,factor=0.035):\n    noise_amp = factor*np.max(data)*rs.normal()\n    data = data + noise_amp*rs.normal(size=data.shape)\n    return data\n\ndef urban_noise(data, sr=sr,factor=0.1):\n    noise_max = 0\n    while noise_max<=0:\n        noisefile = rs.choice(noisefiles)\n        noisefile,sr = librosa.load(noisefile,sr=sr)\n        noisefile,_ = librosa.effects.trim(noisefile,top_db=38)\n        noisefile = np.resize(noisefile,data.shape)\n        noise_max = noisefile.max()\n\n    mixed = noisefile * factor * data.max()/noisefile.max() + data\n    return mixed\n\ndef preprocess(filepath,sr=sr, add_white_noise=False, white_noise_factor=0.035, add_urban_noise=False,\n               urban_noise_factor=0.1,return_wav=False,fixed_start=False):\n    wav,sr = librosa.load(filepath,sr=sr)\n    wav,_ = librosa.effects.trim(wav,top_db=38)\n    size= wav.shape[0]\n    random_start = rs.randint(0,size-trim_length-1)\n    if fixed_start:\n        random_start=fixed_start\n    wav = wav[random_start:random_start+trim_length]\n    if add_white_noise:\n        wav = white_noise(wav,factor=white_noise_factor)\n    if add_urban_noise:\n        wav = urban_noise(wav,sr=sr,factor=urban_noise_factor)        \n    if return_wav:\n        return wav\n    \n    spectrogram =  tf.signal.stft(wav, frame_length=frame_length, fft_length=n_fft,\n                                      frame_step=frame_step)\n    spectrogram = tf.expand_dims(spectrogram, axis=2).numpy()\n    return spectrogram\n\nspecclean = np.abs(preprocess(files[0]))\nprint('spectrum shape', specclean.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:37:24.651543Z","iopub.execute_input":"2022-07-17T07:37:24.651922Z","iopub.status.idle":"2022-07-17T07:37:26.893537Z","shell.execute_reply.started":"2022-07-17T07:37:24.651890Z","shell.execute_reply":"2022-07-17T07:37:26.892299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_test = 100\nfiles = files[:num_test]\ntry:\n    files.remove('/kaggle/input/speech-accent-archive/recordings/recordings/maltese2.mp3')\nexcept:\n    pass\nspectest = preprocess(files[0])\nspectogram_data_clean = np.zeros((len(files),*spectest.shape),dtype=complex)\nspectogram_data_corrupted =np.zeros((len(files),*spectest.shape),dtype=complex)\n\nfor file_ind, afile in enumerate(files):\n    wav,sr = librosa.load(afile,sr=sr)\n    wav,_ = librosa.effects.trim(wav,top_db=38)\n    size= wav.shape[0]\n    random_start = rs.randint(0,size-trim_length-1)\n    spectogram_data_clean[file_ind] = preprocess(afile,fixed_start=random_start)\n    spectogram_data_corrupted[file_ind] = preprocess(afile,add_urban_noise=True,urban_noise_factor=0.6,\n                                                     white_noise_factor=0.035,add_white_noise=True,fixed_start=random_start)\n\n    \nspectogram_data_corrupted_abs = np.abs(spectogram_data_corrupted)\nspectogram_data_clean_abs = np.abs(spectogram_data_clean)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:37:26.896764Z","iopub.execute_input":"2022-07-17T07:37:26.897679Z","iopub.status.idle":"2022-07-17T07:45:59.067477Z","shell.execute_reply.started":"2022-07-17T07:37:26.897636Z","shell.execute_reply":"2022-07-17T07:45:59.065915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"from copy import deepcopy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    BatchNormalization,\n    Conv2D,\n    Conv2DTranspose,\n    MaxPooling2D,\n    Dropout,\n    SpatialDropout2D,\n    UpSampling2D,\n    Input,\n    concatenate,\n    multiply,\n    add,\n    Activation,\n)\n\n\ndef upsample_conv(filters, kernel_size, strides, padding):\n    return Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n\n\ndef upsample_simple(filters, kernel_size, strides, padding):\n    return UpSampling2D(strides)\n\n\ndef attention_gate(inp_1, inp_2, n_intermediate_filters):\n    \"\"\"Attention gate. Compresses both inputs to n_intermediate_filters filters before processing.\n       Implemented as proposed by Oktay et al. in their Attention U-net, see: https://arxiv.org/abs/1804.03999.\n    \"\"\"\n    inp_1_conv = Conv2D(\n        n_intermediate_filters,\n        kernel_size=1,\n        strides=1,\n        padding=\"same\",\n        kernel_initializer=\"he_normal\",\n    )(inp_1)\n    inp_2_conv = Conv2D(\n        n_intermediate_filters,\n        kernel_size=1,\n        strides=1,\n        padding=\"same\",\n        kernel_initializer=\"he_normal\",\n    )(inp_2)\n\n    f = Activation(\"relu\")(add([inp_1_conv, inp_2_conv]))\n    g = Conv2D(\n        filters=1,\n        kernel_size=1,\n        strides=1,\n        padding=\"same\",\n        kernel_initializer=\"he_normal\",\n    )(f)\n    h = Activation(\"sigmoid\")(g)\n    return multiply([inp_1, h])\n\n\ndef attention_concat(conv_below, skip_connection):\n    \"\"\"Performs concatenation of upsampled conv_below with attention gated version of skip-connection\n    \"\"\"\n    below_filters = conv_below.get_shape().as_list()[-1]\n    attention_across = attention_gate(skip_connection, conv_below, below_filters)\n    return concatenate([conv_below, attention_across])\n\n\ndef conv2d_block(\n    inputs,\n    use_batch_norm=True,\n    dropout=0.3,\n    dropout_type=\"spatial\",\n    filters=16,\n    kernel_size=(3, 3),\n    activation=\"relu\",\n    kernel_initializer=\"he_normal\",\n    padding=\"same\",\n):\n\n    if dropout_type == \"spatial\":\n        DO = SpatialDropout2D\n    elif dropout_type == \"standard\":\n        DO = Dropout\n    else:\n        raise ValueError(\n            f\"dropout_type must be one of ['spatial', 'standard'], got {dropout_type}\"\n        )\n\n    c = Conv2D(\n        filters,\n        kernel_size,\n        activation=activation,\n        kernel_initializer=kernel_initializer,\n        padding=padding,\n        use_bias=not use_batch_norm,\n    )(inputs)\n    if use_batch_norm:\n        c = BatchNormalization()(c)\n    if dropout > 0.0:\n        c = DO(dropout)(c)\n    c = Conv2D(\n        filters,\n        kernel_size,\n        activation=activation,\n        kernel_initializer=kernel_initializer,\n        padding=padding,\n        use_bias=not use_batch_norm,\n    )(c)\n    if use_batch_norm:\n        c = BatchNormalization()(c)\n    return c\n\n\ndef custom_unet(\n    input_shape,\n    num_classes=1,\n    activation=\"relu\",\n    use_batch_norm=True,\n    upsample_mode=\"deconv\",  # 'deconv' or 'simple'\n    dropout=0.3,\n    dropout_change_per_layer=0.0,\n    dropout_type=\"spatial\",\n    use_dropout_on_upsampling=False,\n    use_attention=False,\n    filters=16,\n    num_layers=4,\n    output_activation=\"sigmoid\",\n):  # 'sigmoid' or 'softmax'\n\n    if upsample_mode == \"deconv\":\n        upsample = upsample_conv\n    else:\n        upsample = upsample_simple\n\n    # Build U-Net model\n    inputs = Input(input_shape)\n    inputs_copy = tf.identity(inputs)\n    x = inputs / tf.reduce_max(inputs)\n\n    down_layers = []\n    for l in range(num_layers):\n        x = conv2d_block(\n            inputs=x,\n            filters=filters,\n            use_batch_norm=use_batch_norm,\n            dropout=dropout,\n            dropout_type=dropout_type,\n            activation=activation,\n        )\n        down_layers.append(x)\n        x = MaxPooling2D((2, 2))(x)\n        dropout += dropout_change_per_layer\n        filters = filters * 2  # double the number of filters with each layer\n\n    x = conv2d_block(\n        inputs=x,\n        filters=filters,\n        use_batch_norm=use_batch_norm,\n        dropout=dropout,\n        dropout_type=dropout_type,\n        activation=activation,\n    )\n\n    if not use_dropout_on_upsampling:\n        dropout = 0.0\n        dropout_change_per_layer = 0.0\n\n    for conv in reversed(down_layers):\n        filters //= 2  # decreasing number of filters with each layer\n        dropout -= dropout_change_per_layer\n        x = upsample(filters, (2, 2), strides=(2, 2), padding=\"same\")(x)\n        if use_attention:\n            x = attention_concat(conv_below=x, skip_connection=conv)\n        else:\n            x = concatenate([x, conv])\n        x = conv2d_block(\n            inputs=x,\n            filters=filters,\n            use_batch_norm=use_batch_norm,\n            dropout=dropout,\n            dropout_type=dropout_type,\n            activation=activation,\n        )\n\n    output_mask = Conv2D(num_classes, (1, 1), activation=output_activation)(x)\n    outputs = keras.layers.Multiply()([output_mask, inputs_copy])\n    model = Model(inputs=[inputs], outputs=[outputs])\n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:45:59.069246Z","iopub.execute_input":"2022-07-17T07:45:59.069682Z","iopub.status.idle":"2022-07-17T07:45:59.102085Z","shell.execute_reply.started":"2022-07-17T07:45:59.069642Z","shell.execute_reply":"2022-07-17T07:45:59.100952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = custom_unet(\n    input_shape=(256, 128, 1),\n    use_batch_norm=True,\n    num_classes=1,\n    filters=16, \n    num_layers=4,\n    dropout=0.2,\n    output_activation='sigmoid')\n\n\ndef signal_enhancement_loss(y_true, y_pred):\n    mae = tf.abs(y_true - y_pred)\n    speech_loss =  2 * tf.abs(y_true**2 - y_pred*y_true)\n    return tf.reduce_mean(mae, axis=-1) + tf.reduce_mean(speech_loss, axis=-1) # Note the `axis=-1`\n\nmodel.compile(optimizer='adam', loss=signal_enhancement_loss)\nmodel.load_weights('/kaggle/input/speech-mask-model/model_weights_custom_loss2.h5')\n","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:45:59.103532Z","iopub.execute_input":"2022-07-17T07:45:59.104035Z","iopub.status.idle":"2022-07-17T07:45:59.962971Z","shell.execute_reply.started":"2022-07-17T07:45:59.103988Z","shell.execute_reply":"2022-07-17T07:45:59.961806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.utils.vis_utils.plot_model(model,show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:45:59.964865Z","iopub.execute_input":"2022-07-17T07:45:59.965354Z","iopub.status.idle":"2022-07-17T07:46:02.205471Z","shell.execute_reply.started":"2022-07-17T07:45:59.965307Z","shell.execute_reply":"2022-07-17T07:46:02.204487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inspect Results","metadata":{}},{"cell_type":"code","source":"history = model.evaluate(x=spectogram_data_corrupted_abs,y=spectogram_data_clean_abs)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:02.207215Z","iopub.execute_input":"2022-07-17T07:46:02.207752Z","iopub.status.idle":"2022-07-17T07:46:05.821072Z","shell.execute_reply.started":"2022-07-17T07:46:02.207714Z","shell.execute_reply":"2022-07-17T07:46:05.820354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files_to_test = files\npesq_with_noise = np.zeros(len(files_to_test))\npesq_denoised = np.zeros(len(files_to_test))\n\nwav_clean_array =  np.zeros((len(files_to_test),trim_length))\nwav_corrupt_array =  np.zeros((len(files_to_test),trim_length))\nwav_correct_array =  np.zeros((len(files_to_test),trim_length))\nspec_clean_array=  np.zeros((len(files_to_test), 256, 128))\nspec_corrupt_array=  np.zeros((len(files_to_test), 256, 128))\nspec_correct_array=  np.zeros((len(files_to_test), 256, 128))\nloss_with_noise = np.zeros(len(files_to_test))\nloss_denoised = np.zeros(len(files_to_test))\n\nfor ind in range(len(files_to_test)):\n    corr = spectogram_data_corrupted[ind]\n    clean = spectogram_data_clean[ind]\n    corr_wav = tf.signal.inverse_stft(corr[:,:,0], frame_length=frame_length, fft_length=n_fft, frame_step=frame_step)\n    clean_wav = tf.signal.inverse_stft(clean[:,:,0], frame_length=frame_length, fft_length=n_fft, frame_step=frame_step)\n    corr_amp = np.abs(corr)\n    corrected_amp = model.predict(np.expand_dims(corr_amp,0))\n    corrected_spec = corrected_amp * np.exp(1j*np.angle(np.expand_dims(corr,0)))\n    corrected_wav = tf.signal.inverse_stft(corrected_spec[0,:,:,0], frame_length=frame_length, fft_length=n_fft, frame_step=frame_step) \n     \n    pesq_with_noise[ind] = pesq(clean_wav,corr_wav,sr)\n    pesq_denoised[ind] = pesq(clean_wav,corrected_wav,sr)\n    if np.isnan(pesq_denoised[ind]):\n        print('nan detected')\n        break\n    wav_clean_array[ind] = clean_wav\n    wav_corrupt_array[ind] = corr_wav\n    wav_correct_array[ind] = corrected_wav\n    spec_clean_array[ind] = np.abs(clean[:,:,0])\n    spec_corrupt_array[ind] = np.abs(corr[:,:,0])\n    spec_correct_array[ind] = corrected_amp[0,:,:,0]\n    loss_with_noise[ind] = tf.reduce_mean(signal_enhancement_loss(np.abs(clean), corr_amp)).numpy()\n    loss_denoised[ind] = tf.reduce_mean(signal_enhancement_loss(np.abs(clean[:,:,0]), corrected_amp[0,:,:,0])).numpy()\npesq_diff = pesq_denoised-pesq_with_noise\nprint(np.mean(pesq_with_noise), np.mean(pesq_denoised))\nf'{np.mean(pesq_with_noise):.2f}, {np.mean(pesq_denoised):.2f}'","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:05.822396Z","iopub.execute_input":"2022-07-17T07:46:05.822751Z","iopub.status.idle":"2022-07-17T07:46:31.335457Z","shell.execute_reply.started":"2022-07-17T07:46:05.822721Z","shell.execute_reply":"2022-07-17T07:46:31.334434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_dir =f'results_unseen_test'\nos.system(f'rm -r {results_dir}')\nos.mkdir(results_dir)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:31.339137Z","iopub.execute_input":"2022-07-17T07:46:31.339492Z","iopub.status.idle":"2022-07-17T07:46:31.346522Z","shell.execute_reply.started":"2022-07-17T07:46:31.339462Z","shell.execute_reply":"2022-07-17T07:46:31.345746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure()\nplt.title('PESQ improvement')\nplt.hist(pesq_diff);\nplt.xlabel('PESQ corrected - PESQ corrupted')\nplt.ylabel('Number')\nfig.savefig(results_dir+'/pesq_hist', bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:31.347494Z","iopub.execute_input":"2022-07-17T07:46:31.347815Z","iopub.status.idle":"2022-07-17T07:46:31.666483Z","shell.execute_reply.started":"2022-07-17T07:46:31.347788Z","shell.execute_reply":"2022-07-17T07:46:31.665570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind=np.where(pesq_diff==pesq_diff.max())[0][0]\nsf.write(results_dir +'/'+'clean_best.wav',wav_clean_array[ind],sr)\nsf.write(results_dir +'/'+'corrupt_best.wav',wav_corrupt_array[ind],sr)\nsf.write(results_dir +'/'+'correct_best.wav',wav_correct_array[ind],sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:31.667775Z","iopub.execute_input":"2022-07-17T07:46:31.668091Z","iopub.status.idle":"2022-07-17T07:46:31.689228Z","shell.execute_reply.started":"2022-07-17T07:46:31.668064Z","shell.execute_reply":"2022-07-17T07:46:31.688402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(wav_clean_array[ind],rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:31.690586Z","iopub.execute_input":"2022-07-17T07:46:31.691305Z","iopub.status.idle":"2022-07-17T07:46:31.700573Z","shell.execute_reply.started":"2022-07-17T07:46:31.691257Z","shell.execute_reply":"2022-07-17T07:46:31.699630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(wav_corrupt_array[ind],rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:31.701825Z","iopub.execute_input":"2022-07-17T07:46:31.702624Z","iopub.status.idle":"2022-07-17T07:46:31.710688Z","shell.execute_reply.started":"2022-07-17T07:46:31.702564Z","shell.execute_reply":"2022-07-17T07:46:31.709813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(wav_correct_array[ind],rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:31.712102Z","iopub.execute_input":"2022-07-17T07:46:31.713196Z","iopub.status.idle":"2022-07-17T07:46:31.722299Z","shell.execute_reply.started":"2022-07-17T07:46:31.713161Z","shell.execute_reply":"2022-07-17T07:46:31.721569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind=np.where(pesq_diff==pesq_diff.max())[0][0]\nfig,axes = plt.subplots(ncols=3,figsize=(20,10))\nvmax=spec_clean_array[ind].max()/3\nvmin=0\nplt.subplot(1,3,1)\nplt.title('Ground Truth')\nplt.imshow(spec_clean_array[ind], origin='lower',vmax=vmax,vmin=vmin)\nplt.subplot(1,3,2)\nplt.title('Ground Truth + Noise')\nplt.imshow(spec_corrupt_array[ind], origin='lower',vmax=vmax,vmin=vmin)\nplt.subplot(1,3,3)\nplt.title('Corrected')\nplt.imshow(spec_correct_array[ind], origin='lower',vmax=vmax,vmin=vmin)\nplt.colorbar()\nfig.savefig(results_dir+'/best_spec.png', bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:31.723476Z","iopub.execute_input":"2022-07-17T07:46:31.724068Z","iopub.status.idle":"2022-07-17T07:46:32.919605Z","shell.execute_reply.started":"2022-07-17T07:46:31.724036Z","shell.execute_reply":"2022-07-17T07:46:32.918515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind=np.where(pesq_diff==pesq_diff.min())[0][0]\nsf.write(results_dir +'/'+'clean_worst.wav',wav_clean_array[ind],sr)\nsf.write(results_dir +'/'+'corrupt_worst.wav',wav_corrupt_array[ind],sr)\nsf.write(results_dir +'/'+'correct_worst.wav',wav_correct_array[ind],sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:32.921286Z","iopub.execute_input":"2022-07-17T07:46:32.921867Z","iopub.status.idle":"2022-07-17T07:46:32.946058Z","shell.execute_reply.started":"2022-07-17T07:46:32.921820Z","shell.execute_reply":"2022-07-17T07:46:32.945065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(wav_clean_array[ind],rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:32.947344Z","iopub.execute_input":"2022-07-17T07:46:32.947882Z","iopub.status.idle":"2022-07-17T07:46:32.955237Z","shell.execute_reply.started":"2022-07-17T07:46:32.947850Z","shell.execute_reply":"2022-07-17T07:46:32.954492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(wav_corrupt_array[ind],rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:32.956331Z","iopub.execute_input":"2022-07-17T07:46:32.956827Z","iopub.status.idle":"2022-07-17T07:46:32.967613Z","shell.execute_reply.started":"2022-07-17T07:46:32.956783Z","shell.execute_reply":"2022-07-17T07:46:32.966790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(wav_correct_array[ind],rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:32.968793Z","iopub.execute_input":"2022-07-17T07:46:32.969939Z","iopub.status.idle":"2022-07-17T07:46:32.980119Z","shell.execute_reply.started":"2022-07-17T07:46:32.969899Z","shell.execute_reply":"2022-07-17T07:46:32.978950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind=np.where(pesq_diff==pesq_diff.min())[0][0]\nfig,axes = plt.subplots(ncols=3,figsize=(20,10))\nvmax=spec_clean_array[ind].max()/3\nvmin=0\nplt.subplot(1,3,1)\nplt.title('Ground Truth')\nplt.imshow(spec_clean_array[ind], origin='lower',vmax=vmax,vmin=vmin)\nplt.subplot(1,3,2)\nplt.title('Ground Truth + Noise')\nplt.imshow(spec_corrupt_array[ind], origin='lower',vmax=vmax,vmin=vmin)\nplt.subplot(1,3,3)\nplt.title('Corrected')\nplt.imshow(spec_correct_array[ind], origin='lower',vmax=vmax,vmin=vmin)\nplt.colorbar()\nfig.savefig(results_dir+'/worst_spec.png', bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:32.981679Z","iopub.execute_input":"2022-07-17T07:46:32.982285Z","iopub.status.idle":"2022-07-17T07:46:34.220007Z","shell.execute_reply.started":"2022-07-17T07:46:32.982246Z","shell.execute_reply":"2022-07-17T07:46:34.218945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.system('rm unseentest_results.tar.gz')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:34.221492Z","iopub.execute_input":"2022-07-17T07:46:34.221875Z","iopub.status.idle":"2022-07-17T07:46:34.231108Z","shell.execute_reply.started":"2022-07-17T07:46:34.221844Z","shell.execute_reply":"2022-07-17T07:46:34.230088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.system(f'tar -cvzf unseentest_results.tar.gz {results_dir}')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T07:46:34.232656Z","iopub.execute_input":"2022-07-17T07:46:34.233077Z","iopub.status.idle":"2022-07-17T07:46:34.295553Z","shell.execute_reply.started":"2022-07-17T07:46:34.233039Z","shell.execute_reply":"2022-07-17T07:46:34.294534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}