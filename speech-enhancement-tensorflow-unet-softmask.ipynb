{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Speech Enhancement Algorithm based on Tensorflow and Keras\n**\n\n**Data**\nI mixed clean speech data from RAVDESS dataset with noise data from the UrbanSound8K dataset. As tensorflow was unable to read the raw files, I rewrote the original datasets locally and uploaded them back onto Kaggle. I also downsampled the datasets back to 8K - this captures most of the Speech signal. \n\n\nThis is quite a limited dataset for speech enhancement. My goal for this project was primarily to check how well this algorithm generally to a completely different unseen dataset.\n\n**ALGORITHM** \nThis algorithm takes an Short Time Fourier Transform (STFT) of the audio files to return the transformed signal as a 2D array of dimensions time and frequency. The transformed signal is complex, but the algorithm only operates on the amplitude. The amplitude is passed through a softmask + UNet algorithm. This is a re-worked version from https://github.com/karolzak/keras-unet where I added the softmask component.  \n\n**Loss Function**\n\nI started with just an MSE loss between the clean speech amplitude and the reconstructed speech amplitude, however, this had the problem that it treated keeping noise equivalent to losing speech. As it is much more important to keep speech than to reduce noise, I created a custom loss function which features both a MSE term and another term which focuses specifically on \n\n**METRIC**\nThe PESQ metric is used in the industry as an automatic way to measure speech quality. It is not without it's problems but it still works as a first pass. I would advise you to listen to the output files (clean, corrupted and corrected) to get a better sense of how the algorithm performs.\n\n**Unseen data test**\nIn the Notebook below I test the model on a completely different dataset featuring 150+ different english accents mixed with hospital noise.\nhttps://www.kaggle.com/tariqblecher/speech-enhancement-unseen-data-test\n\n**Results**\nOn the validation data, we saw an average increase of +0.44 in the PESQ score (from 2.08 to 2.52)\nIn the unseen test data, we saw an average increase of +0.17 in the PESQ score (from 2.13 to 2.30)\n\nPlease cite this notebook if you use it.\n\n**contact** : tariq.blecher@gmail.com ","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install pypesq","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:40:35.095251Z","iopub.execute_input":"2022-07-17T06:40:35.095677Z","iopub.status.idle":"2022-07-17T06:40:51.627092Z","shell.execute_reply.started":"2022-07-17T06:40:35.095592Z","shell.execute_reply":"2022-07-17T06:40:51.626139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport sys\nimport librosa\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nimport tensorflow_io as tfio\nimport keras\nfrom keras.models import Sequential\nimport tensorflow_io as tfio\nimport warnings\nimport glob\nfrom keras.callbacks import ModelCheckpoint\nfrom pypesq import pesq\nimport soundfile as sf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-17T06:40:51.629005Z","iopub.execute_input":"2022-07-17T06:40:51.629585Z","iopub.status.idle":"2022-07-17T06:41:00.111232Z","shell.execute_reply.started":"2022-07-17T06:40:51.629544Z","shell.execute_reply":"2022-07-17T06:41:00.110340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data and Preprocess","metadata":{}},{"cell_type":"code","source":"sr=8000\nspeech_length_pix_sec=27e-3\ntotal_length = 3.6\ntrim_length = 28305\nn_fft=255\nframe_length=255\nframe_step = 110\n\nnoisefiles = glob.glob('/kaggle/input/urban-sound-8k/**/*.wav')\nfiles= glob.glob('/kaggle/input/ravdess-8k/**/*.wav')\nprint(len(files),'clean data files')\nprint('Should be similar value to trim length', total_length*sr,trim_length)\nprint('Should be similar value to n_fft',n_fft, int(speech_length_pix_sec*sr))\n\n@tf.function\ndef load_wav(filename):\n    file_contents = tf.io.read_file(filename)\n    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\n    wav = tf.squeeze(wav, axis=-1)\n#     sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n#     wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=sr)\n    return wav\n\n@tf.function\ndef preprocess_tf(filepath):\n    wav = load_wav(filepath)\n    wav = wav[:trim_length]\n    zero_padding = tf.zeros([trim_length] - tf.shape(wav), dtype=tf.float32)\n    wav = tf.concat([zero_padding, wav],0)\n    return wav\n\n@tf.function\ndef white_noise(data,factor=0.03):\n    noise_amp = factor*tf.reduce_max(data)*tf.random.normal(shape=(1,))\n    corr_data = data + noise_amp*tf.random.normal(shape=tf.shape(data))\n    return corr_data, data\n\n@tf.function\ndef urban_noise(corr_data, data, factor=0.4,sr=sr):\n    noisefile = tf.gather(noisefiles,tf.random.uniform((),0, len(noisefiles)-1,dtype=tf.int32))\n    noisefile  = load_wav(noisefile)\n    mixed = noisefile * factor * tf.reduce_max(corr_data)/tf.reduce_max(noisefile) + corr_data \n    return mixed, data\n\n@tf.function\ndef convert_to_spectrogram(wav_corr, wavclean):\n    spectrogram_corr = tf.signal.stft(wav_corr, frame_length=frame_length, fft_length=n_fft,\n                                      frame_step=frame_step)\n    spectrogram = tf.signal.stft(wavclean, frame_length=frame_length, fft_length=n_fft,\n                                      frame_step=frame_step)\n    return spectrogram_corr, spectrogram\n\n@tf.function\ndef spectrogram_abs(spectrogram_corr, spectrogram):\n    spectrogram = tf.abs(spectrogram)\n    spectrogram_corr = tf.abs(spectrogram_corr)\n    return spectrogram_corr, spectrogram\n\n@tf.function\ndef augment(spectrogram_corr, spectrogram):\n    spectrogram_corr = tfio.audio.freq_mask(spectrogram_corr, 10)\n    spectrogram_corr = tfio.audio.time_mask(spectrogram_corr, 20)\n    return spectrogram_corr, spectrogram\n\n@tf.function\ndef expand_dims(spectrogram_corr, spectrogram):\n    spectrogram_corr = tf.expand_dims(spectrogram_corr, axis=2)\n    spectrogram = tf.expand_dims(spectrogram, axis=2)\n    return spectrogram_corr, spectrogram","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:41:00.112821Z","iopub.execute_input":"2022-07-17T06:41:00.113473Z","iopub.status.idle":"2022-07-17T06:41:00.522680Z","shell.execute_reply.started":"2022-07-17T06:41:00.113437Z","shell.execute_reply":"2022-07-17T06:41:00.521714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=16\ntrain_val_split_ratio = 0.2\nsplit_index = int(len(files)*train_val_split_ratio)\ntrain_files = files[split_index:]\nval_files = files[:split_index]\n\ndef configure_dataset(files, train=True):\n    dataset = tf.data.Dataset.from_tensor_slices(files)\n    dataset = dataset.map(load_wav, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.map(white_noise, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.map(urban_noise, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.map(convert_to_spectrogram, num_parallel_calls=tf.data.AUTOTUNE)\n    if not train:\n        dataset = dataset.map(expand_dims, num_parallel_calls=tf.data.AUTOTUNE)\n\n    if train:\n        dataset = dataset.map(spectrogram_abs, num_parallel_calls=tf.data.AUTOTUNE)\n        dataset = dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n        dataset = dataset.map(expand_dims, num_parallel_calls=tf.data.AUTOTUNE)\n        \n        dataset = dataset.batch(batch_size)\n        dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset\ntrain_dataset = configure_dataset(train_files)\nval_dataset = configure_dataset(val_files)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:41:00.524058Z","iopub.execute_input":"2022-07-17T06:41:00.524500Z","iopub.status.idle":"2022-07-17T06:41:04.513374Z","shell.execute_reply.started":"2022-07-17T06:41:00.524463Z","shell.execute_reply":"2022-07-17T06:41:04.512629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create and Run Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    BatchNormalization,\n    Conv2D,\n    Conv2DTranspose,\n    MaxPooling2D,\n    Dropout,\n    SpatialDropout2D,\n    UpSampling2D,\n    Input,\n    concatenate,\n    multiply,\n    add,\n    Activation,\n)\n\n\ndef upsample_conv(filters, kernel_size, strides, padding):\n    return Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n\n\ndef upsample_simple(filters, kernel_size, strides, padding):\n    return UpSampling2D(strides)\n\n\ndef attention_gate(inp_1, inp_2, n_intermediate_filters):\n    \"\"\"Attention gate. Compresses both inputs to n_intermediate_filters filters before processing.\n       Implemented as proposed by Oktay et al. in their Attention U-net, see: https://arxiv.org/abs/1804.03999.\n    \"\"\"\n    inp_1_conv = Conv2D(\n        n_intermediate_filters,\n        kernel_size=1,\n        strides=1,\n        padding=\"same\",\n        kernel_initializer=\"he_normal\",\n    )(inp_1)\n    inp_2_conv = Conv2D(\n        n_intermediate_filters,\n        kernel_size=1,\n        strides=1,\n        padding=\"same\",\n        kernel_initializer=\"he_normal\",\n    )(inp_2)\n\n    f = Activation(\"relu\")(add([inp_1_conv, inp_2_conv]))\n    g = Conv2D(\n        filters=1,\n        kernel_size=1,\n        strides=1,\n        padding=\"same\",\n        kernel_initializer=\"he_normal\",\n    )(f)\n    h = Activation(\"sigmoid\")(g)\n    return multiply([inp_1, h])\n\n\ndef attention_concat(conv_below, skip_connection):\n    \"\"\"Performs concatenation of upsampled conv_below with attention gated version of skip-connection\n    \"\"\"\n    below_filters = conv_below.get_shape().as_list()[-1]\n    attention_across = attention_gate(skip_connection, conv_below, below_filters)\n    return concatenate([conv_below, attention_across])\n\n\ndef conv2d_block(\n    inputs,\n    use_batch_norm=True,\n    dropout=0.3,\n    dropout_type=\"spatial\",\n    filters=16,\n    kernel_size=(3, 3),\n    activation=\"relu\",\n    kernel_initializer=\"he_normal\",\n    padding=\"same\",\n):\n\n    if dropout_type == \"spatial\":\n        DO = SpatialDropout2D\n    elif dropout_type == \"standard\":\n        DO = Dropout\n    else:\n        raise ValueError(\n            f\"dropout_type must be one of ['spatial', 'standard'], got {dropout_type}\"\n        )\n\n    c = Conv2D(\n        filters,\n        kernel_size,\n        activation=activation,\n        kernel_initializer=kernel_initializer,\n        padding=padding,\n        use_bias=not use_batch_norm,\n    )(inputs)\n    if use_batch_norm:\n        c = BatchNormalization()(c)\n    if dropout > 0.0:\n        c = DO(dropout)(c)\n    c = Conv2D(\n        filters,\n        kernel_size,\n        activation=activation,\n        kernel_initializer=kernel_initializer,\n        padding=padding,\n        use_bias=not use_batch_norm,\n    )(c)\n    if use_batch_norm:\n        c = BatchNormalization()(c)\n    return c\n\n\ndef custom_unet(\n    input_shape,\n    num_classes=1,\n    activation=\"relu\",\n    use_batch_norm=True,\n    upsample_mode=\"deconv\",  # 'deconv' or 'simple'\n    dropout=0.3,\n    dropout_change_per_layer=0.0,\n    dropout_type=\"spatial\",\n    use_dropout_on_upsampling=False,\n    use_attention=False,\n    filters=16,\n    num_layers=4,\n    output_activation=\"sigmoid\",\n):  # 'sigmoid' or 'softmax'\n\n    if upsample_mode == \"deconv\":\n        upsample = upsample_conv\n    else:\n        upsample = upsample_simple\n\n    # Build U-Net model\n    inputs = Input(input_shape)\n    inputs_copy = tf.identity(inputs)\n    x = inputs / tf.reduce_max(inputs)\n\n    down_layers = []\n    for l in range(num_layers):\n        x = conv2d_block(\n            inputs=x,\n            filters=filters,\n            use_batch_norm=use_batch_norm,\n            dropout=dropout,\n            dropout_type=dropout_type,\n            activation=activation,\n        )\n        down_layers.append(x)\n        x = MaxPooling2D((2, 2))(x)\n        dropout += dropout_change_per_layer\n        filters = filters * 2  # double the number of filters with each layer\n\n    x = conv2d_block(\n        inputs=x,\n        filters=filters,\n        use_batch_norm=use_batch_norm,\n        dropout=dropout,\n        dropout_type=dropout_type,\n        activation=activation,\n    )\n\n    if not use_dropout_on_upsampling:\n        dropout = 0.0\n        dropout_change_per_layer = 0.0\n\n    for conv in reversed(down_layers):\n        filters //= 2  # decreasing number of filters with each layer\n        dropout -= dropout_change_per_layer\n        x = upsample(filters, (2, 2), strides=(2, 2), padding=\"same\")(x)\n        if use_attention:\n            x = attention_concat(conv_below=x, skip_connection=conv)\n        else:\n            x = concatenate([x, conv])\n        x = conv2d_block(\n            inputs=x,\n            filters=filters,\n            use_batch_norm=use_batch_norm,\n            dropout=dropout,\n            dropout_type=dropout_type,\n            activation=activation,\n        )\n\n    output_mask = Conv2D(num_classes, (1, 1), activation=output_activation)(x)\n    outputs = keras.layers.Multiply()([output_mask, inputs_copy])\n    model = Model(inputs=[inputs], outputs=[outputs])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:41:04.522884Z","iopub.execute_input":"2022-07-17T06:41:04.523685Z","iopub.status.idle":"2022-07-17T06:41:04.550149Z","shell.execute_reply.started":"2022-07-17T06:41:04.523647Z","shell.execute_reply":"2022-07-17T06:41:04.549363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = custom_unet(\n    input_shape=(256, 128, 1),\n    use_batch_norm=True,\n    num_classes=1,\n    filters=16, \n    num_layers=4,\n    dropout=0.2,\n    output_activation='sigmoid')\n\nmodel_filename = 'model_weights.h5'\ncallback_checkpoint = ModelCheckpoint(\n    model_filename, \n    verbose=1, \n    monitor='val_loss',\n    save_weights_only=True,\n    save_best_only=True)\n\ncallback_early_stop =tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0,\n    patience=4,\n    mode=\"auto\",\n    restore_best_weights=True,\n)\n\ndef signal_enhancement_loss(y_true, y_pred):\n    mae = tf.abs(y_true - y_pred)\n    speech_loss =  2 * tf.abs(y_true**2 - y_pred*y_true)\n    return tf.reduce_mean(mae, axis=-1) + tf.reduce_mean(speech_loss, axis=-1) # Note the `axis=-1`\n\nmodel.compile(optimizer='adam', loss=signal_enhancement_loss)\nmodel.load_weights('/kaggle/input/speech-mask-model/model_weights_custom_loss2.h5')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:41:04.551499Z","iopub.execute_input":"2022-07-17T06:41:04.551881Z","iopub.status.idle":"2022-07-17T06:41:05.222779Z","shell.execute_reply.started":"2022-07-17T06:41:04.551845Z","shell.execute_reply":"2022-07-17T06:41:05.222001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keras.utils.vis_utils.plot_model(model,show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:41:05.223974Z","iopub.execute_input":"2022-07-17T06:41:05.224320Z","iopub.status.idle":"2022-07-17T06:41:05.228294Z","shell.execute_reply.started":"2022-07-17T06:41:05.224284Z","shell.execute_reply":"2022-07-17T06:41:05.227597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run Model \n(Commented out as I am reading in a previously run model)","metadata":{}},{"cell_type":"code","source":"# history = model.fit(train_dataset,epochs=30,shuffle=True, validation_data=val_dataset,\n#                     callbacks=[callback_checkpoint, callback_early_stop])","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:41:05.229335Z","iopub.execute_input":"2022-07-17T06:41:05.230102Z","iopub.status.idle":"2022-07-17T06:41:05.238246Z","shell.execute_reply.started":"2022-07-17T06:41:05.230052Z","shell.execute_reply":"2022-07-17T06:41:05.237492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inspect Results","metadata":{}},{"cell_type":"code","source":"trim_length = 28305\nfiles_to_test = val_files\ntest_dataset = configure_dataset(files_to_test,train=False)\nnum = test_dataset.as_numpy_iterator()\nmae = tf.keras.losses.MeanAbsoluteError()\npesq_with_noise = np.zeros(len(files_to_test))\npesq_denoised = np.zeros(len(files_to_test))\n\nwav_clean_array =  np.zeros((len(files_to_test),trim_length))\nwav_corrupt_array =  np.zeros((len(files_to_test),trim_length))\nwav_correct_array =  np.zeros((len(files_to_test),trim_length))\nspec_clean_array=  np.zeros((len(files_to_test), 256, 128))\nspec_corrupt_array=  np.zeros((len(files_to_test), 256, 128))\nspec_correct_array=  np.zeros((len(files_to_test), 256, 128))\nloss_with_noise = np.zeros(len(files_to_test))\nloss_denoised = np.zeros(len(files_to_test))\n\n\nfor ind in range(len(files_to_test)):\n    corr, clean = num.next()\n    corr_wav = tf.signal.inverse_stft(corr[:,:,0], frame_length=frame_length, fft_length=n_fft, frame_step=frame_step)\n    clean_wav = tf.signal.inverse_stft(clean[:,:,0], frame_length=frame_length, fft_length=n_fft, frame_step=frame_step)\n    corr_amp = np.abs(corr)\n    corrected_amp = model.predict(np.expand_dims(corr_amp,0))\n    corrected_spec = corrected_amp * np.exp(1j*np.angle(np.expand_dims(corr,0)))\n    corrected_wav = tf.signal.inverse_stft(corrected_spec[0,:,:,0], frame_length=frame_length, fft_length=n_fft, frame_step=frame_step) \n     \n    pesq_with_noise[ind] = pesq(clean_wav,corr_wav,sr)\n    pesq_denoised[ind] = pesq(clean_wav,corrected_wav,sr)\n    wav_clean_array[ind] = clean_wav\n    wav_corrupt_array[ind] = corr_wav\n    wav_correct_array[ind] = corrected_wav\n    spec_clean_array[ind] = np.abs(clean[:,:,0])\n    spec_corrupt_array[ind] = np.abs(corr[:,:,0])\n    spec_correct_array[ind] = corrected_amp[0,:,:,0]\n    loss_with_noise[ind] = tf.reduce_mean(signal_enhancement_loss(np.abs(clean), corr_amp)).numpy()\n    loss_denoised[ind] =tf.reduce_mean(signal_enhancement_loss(np.abs(clean[:,:,0]), corrected_amp[0,:,:,0])).numpy()\n\npesq_diff = pesq_denoised - pesq_with_noise\n\nprint(np.mean(pesq_with_noise), np.mean(pesq_denoised),pesq_diff.mean())\n\nf'{np.mean(pesq_with_noise):.2f}, {np.mean(pesq_denoised):.2f}'","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:41:05.239496Z","iopub.execute_input":"2022-07-17T06:41:05.239906Z","iopub.status.idle":"2022-07-17T06:42:06.277023Z","shell.execute_reply.started":"2022-07-17T06:41:05.239868Z","shell.execute_reply":"2022-07-17T06:42:06.275986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.system('rm -r val_results')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:42:06.278453Z","iopub.execute_input":"2022-07-17T06:42:06.279992Z","iopub.status.idle":"2022-07-17T06:42:06.293512Z","shell.execute_reply.started":"2022-07-17T06:42:06.279953Z","shell.execute_reply":"2022-07-17T06:42:06.292650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_dir = 'val_results'\nos.mkdir(results_dir)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:42:06.294628Z","iopub.execute_input":"2022-07-17T06:42:06.295106Z","iopub.status.idle":"2022-07-17T06:42:06.310823Z","shell.execute_reply.started":"2022-07-17T06:42:06.295058Z","shell.execute_reply":"2022-07-17T06:42:06.309892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure()\nplt.title('PESQ improvement')\nplt.hist(pesq_diff);\nplt.xlabel('PESQ corrected - PESQ corrupted')\nplt.ylabel('Number')\nfig.savefig(results_dir+'/pesq_hist', bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:42:06.312269Z","iopub.execute_input":"2022-07-17T06:42:06.312684Z","iopub.status.idle":"2022-07-17T06:42:06.628313Z","shell.execute_reply.started":"2022-07-17T06:42:06.312633Z","shell.execute_reply":"2022-07-17T06:42:06.627564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind=np.where(pesq_diff==pesq_diff.max())[0][0]\nsf.write(results_dir +'/'+'clean_best_pesq_improvement.wav',wav_clean_array[ind],sr)\nsf.write(results_dir +'/'+'corrupt_best_pesq_improvement.wav',wav_corrupt_array[ind],sr)\nsf.write(results_dir +'/'+'correct_best_pesq_improvement.wav',wav_correct_array[ind],sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:42:06.631554Z","iopub.execute_input":"2022-07-17T06:42:06.631844Z","iopub.status.idle":"2022-07-17T06:42:06.652567Z","shell.execute_reply.started":"2022-07-17T06:42:06.631818Z","shell.execute_reply":"2022-07-17T06:42:06.651719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(wav_clean_array[ind],rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:42:06.653860Z","iopub.execute_input":"2022-07-17T06:42:06.654266Z","iopub.status.idle":"2022-07-17T06:42:06.669383Z","shell.execute_reply.started":"2022-07-17T06:42:06.654230Z","shell.execute_reply":"2022-07-17T06:42:06.667616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(wav_corrupt_array[ind],rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:47:40.828810Z","iopub.execute_input":"2022-07-17T06:47:40.829622Z","iopub.status.idle":"2022-07-17T06:47:40.838502Z","shell.execute_reply.started":"2022-07-17T06:47:40.829587Z","shell.execute_reply":"2022-07-17T06:47:40.837721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(wav_correct_array[ind],rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:47:41.183679Z","iopub.execute_input":"2022-07-17T06:47:41.184044Z","iopub.status.idle":"2022-07-17T06:47:41.192929Z","shell.execute_reply.started":"2022-07-17T06:47:41.184013Z","shell.execute_reply":"2022-07-17T06:47:41.192181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind=np.where(pesq_diff==pesq_diff.min())[0][0]\nsf.write(results_dir +'/'+'clean_worst_pesq_improvement.wav',wav_clean_array[ind],sr)\nsf.write(results_dir +'/'+'corrupt_worst_pesq_improvement.wav',wav_corrupt_array[ind],sr)\nsf.write(results_dir +'/'+'correct_worst_pesq_improvement.wav',wav_correct_array[ind],sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:42:06.692763Z","iopub.execute_input":"2022-07-17T06:42:06.693161Z","iopub.status.idle":"2022-07-17T06:42:06.716279Z","shell.execute_reply.started":"2022-07-17T06:42:06.693128Z","shell.execute_reply":"2022-07-17T06:42:06.715631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(wav_clean_array[ind],rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:42:06.717449Z","iopub.execute_input":"2022-07-17T06:42:06.717811Z","iopub.status.idle":"2022-07-17T06:42:06.725529Z","shell.execute_reply.started":"2022-07-17T06:42:06.717776Z","shell.execute_reply":"2022-07-17T06:42:06.724870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(wav_corrupt_array[ind],rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:42:06.726661Z","iopub.execute_input":"2022-07-17T06:42:06.727291Z","iopub.status.idle":"2022-07-17T06:42:06.735948Z","shell.execute_reply.started":"2022-07-17T06:42:06.727256Z","shell.execute_reply":"2022-07-17T06:42:06.735273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(wav_correct_array[ind],rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:42:06.737024Z","iopub.execute_input":"2022-07-17T06:42:06.737844Z","iopub.status.idle":"2022-07-17T06:42:06.748242Z","shell.execute_reply.started":"2022-07-17T06:42:06.737811Z","shell.execute_reply":"2022-07-17T06:42:06.747401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind=np.where(pesq_diff==pesq_diff.max())[0][0]\nfig,axes = plt.subplots(ncols=3,figsize=(20,10))\nvmax=spec_clean_array[ind].max()/3\nvmin=0\nplt.subplot(1,3,1)\nplt.title('Ground Truth')\nplt.imshow(spec_clean_array[ind], origin='lower',vmax=vmax,vmin=vmin)\nplt.subplot(1,3,2)\nplt.title('Ground Truth + Noise')\nplt.imshow(spec_corrupt_array[ind], origin='lower',vmax=vmax,vmin=vmin)\nplt.subplot(1,3,3)\nplt.title('Corrected')\nplt.imshow(spec_correct_array[ind], origin='lower',vmax=vmax,vmin=vmin)\nplt.colorbar()\nfig.savefig(results_dir+'/best_spec.png', bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:42:06.749413Z","iopub.execute_input":"2022-07-17T06:42:06.750431Z","iopub.status.idle":"2022-07-17T06:42:07.778965Z","shell.execute_reply.started":"2022-07-17T06:42:06.750395Z","shell.execute_reply":"2022-07-17T06:42:07.778214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind=np.where(pesq_diff==pesq_diff.min())[0][0]\nfig,axes = plt.subplots(ncols=3,figsize=(20,10))\nvmax=spec_clean_array[ind].max()/3\nvmin=0\nplt.subplot(1,3,1)\nplt.title('Ground Truth')\nplt.imshow(spec_clean_array[ind], origin='lower',vmax=vmax,vmin=vmin)\nplt.subplot(1,3,2)\nplt.title('Ground Truth + Noise')\nplt.imshow(spec_corrupt_array[ind], origin='lower',vmax=vmax,vmin=vmin)\nplt.subplot(1,3,3)\nplt.title('Corrected')\nplt.imshow(spec_correct_array[ind], origin='lower',vmax=vmax,vmin=vmin)\nplt.colorbar()\nfig.savefig(results_dir+'/worst_spec.png', bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:42:07.780324Z","iopub.execute_input":"2022-07-17T06:42:07.780890Z","iopub.status.idle":"2022-07-17T06:42:08.722303Z","shell.execute_reply.started":"2022-07-17T06:42:07.780850Z","shell.execute_reply":"2022-07-17T06:42:08.721417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ = np.expand_dims(corr_amp,0)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:42:08.723809Z","iopub.execute_input":"2022-07-17T06:42:08.724161Z","iopub.status.idle":"2022-07-17T06:42:08.729167Z","shell.execute_reply.started":"2022-07-17T06:42:08.724129Z","shell.execute_reply":"2022-07-17T06:42:08.728056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%timeit -n 50\ntest = model.predict(test_)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:42:08.730524Z","iopub.execute_input":"2022-07-17T06:42:08.730912Z","iopub.status.idle":"2022-07-17T06:42:22.076729Z","shell.execute_reply.started":"2022-07-17T06:42:08.730874Z","shell.execute_reply":"2022-07-17T06:42:22.075895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.system(f'tar -cvzf train_val_results.tar.gz {results_dir}')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T06:42:22.078154Z","iopub.execute_input":"2022-07-17T06:42:22.078658Z","iopub.status.idle":"2022-07-17T06:42:22.139414Z","shell.execute_reply.started":"2022-07-17T06:42:22.078621Z","shell.execute_reply":"2022-07-17T06:42:22.138589Z"},"trusted":true},"execution_count":null,"outputs":[]}]}